{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM07qwK4PFApDVVYv45x3nD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LydiaDEdou/projet/blob/main/spam_message_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sujet : spam message classification**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kq5QZZzlytpr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importation des bibliotheque**"
      ],
      "metadata": {
        "id": "8HcCZ4MRxBXA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import spacy\n",
        "from sklearn.feature_selection import SelectKBest, chi2"
      ],
      "metadata": {
        "id": "a7jvpcGPsTUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Télécharger les stopwords Anglais\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "# Charger le modèle spaCy pour la lemmatisation en français\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wHBLOjOsgOC",
        "outputId": "79bb2944-3786-410a-b620-aba41c52407b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Choix des étapes de prétraitement pour la classification d'email avec Naive Bayes**\n",
        "\n",
        "Chaque étape de prétraitement a un rôle spécifique, et l'ordre est essentiel pour produire une représentation textuelle propre et cohérente. Voici les étapes, de leur ordre recommandé, et des explications de leur utilité dans la classification de spam/ham que j'ai utlisé.\n",
        "\n",
        "\n",
        "*   **Lowercasing **: Convertissez simplement le en minuscules avec .lower()\n",
        "*   **Gestion des entités spéciales (Emails, Numéros de téléphone)**\n",
        "        **Pourquoi :** Les spams contiennent souvent des informations comme des numéros de téléphone ou des adresses email. Remplacer ces entités par des tokens génériques ([EMAIL], [PHONE]) permet au modèle de reconnaître ces éléments sans surcharger le vocabulaire.\n",
        "        **Recommandé :** Oui, surtout pour des tâches de spam où ces entités peuvent indiquer un spam potentiel.\n",
        "\n",
        "\n",
        "\n",
        "*  **Suppression de la ponctuation:** cela permet de simplifier la représentation textuelle.\n",
        "*  **Tokenization:** Diviser le texte en unités individuelles appelées tokens (mots en général).\n",
        "\n",
        "\n",
        "*  **Suppression des stopwords** Éliminer les mots courants (comme \"le\", \"de\", \"et\" en anglais) qui n’apportent pas de valeur informative dans la classification.\n",
        "* **Lemmatisation**: Effectuez cette étape après les autres prétraitements pour travailler sur les mots nettoyés et filtrés.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0CHiO_0VtdkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fonction de prétraitement avec gestion des entités spéciales\n",
        "def preprocess_text(text):\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "# Remplacement des entités spécifiques\n",
        "    text = re.sub(r'\\S+@\\S+', ' [EMAIL] ', text)       # Remplace les emails\n",
        "    text = re.sub(r'\\b\\d{10}\\b', ' [PHONE] ', text)    # Remplace les numéros de téléphone\n",
        "    text = re.sub(r'http\\S+|www\\S+', ' [URL] ', text)  # Remplace les URL\n",
        "    text = re.sub(r'\\$\\d+|\\d+\\s?€', ' [MONEY] ', text) # Remplace les montants en $ ou €\n",
        "    text = re.sub(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b', ' [DATE] ', text) # Remplace les dates\n",
        "    text = re.sub(r'\\b\\d+\\b', ' [NUMBER] ', text)      # Remplace les séquences de chiffres\n",
        "    text = re.sub(r'\\b\\d{1,2}(:\\d{2})?\\s?(AM|PM|am|pm)?\\b', ' [TIME] ', text) # Remplace les heures\n",
        "    text = re.sub(r'@\\w+', ' [MENTION] ', text)        # Remplace les mentions\n",
        "    text = re.sub(r'#\\w+', ' [HASHTAG] ', text)        # Remplace les hashtags\n",
        "    text = re.sub(r'[&%$#@*!^]', ' [SYMBOL] ', text)   # Remplace les symboles spéciaux\n",
        "\n",
        "\n",
        "    # Suppression de la ponctuation\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    # Tokenization et suppression des stopwords\n",
        "    tokens = [word for word in text.split() if word not in stop_words]\n",
        "\n",
        "    # Lemmatisation\n",
        "    doc = nlp(\" \".join(tokens))\n",
        "    tokens = [token.lemma_ for token in doc]\n",
        "\n",
        "    return \" \".join(tokens)\n"
      ],
      "metadata": {
        "id": "P2M8RBKUsx-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Charger les données**"
      ],
      "metadata": {
        "id": "dZ9D5wtlxS67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger le dataset\n",
        "df = pd.read_csv(\"/content/MyBalancedSpamClassificationDataset.csv\")\n",
        "df['message'] = df['Message'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "HXuipwEHs6Bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Diviser les données en ensembles d'entraînement et de test\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "mOspe9titKde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Construire la pipeline NLP**\n",
        "\n",
        "La vectorisation des textes avec TfidfVectorizer.\n",
        "L'entraînement du modèle Multinomial Naive Bayes."
      ],
      "metadata": {
        "id": "jMIaMSfRxgIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Ajout de n-grams dans le vectorizer\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(ngram_range=(1, 2))),  # Utilisation de bigrams\n",
        "    ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vsGLeibOtNWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Évaluer le modèle**"
      ],
      "metadata": {
        "id": "Sw2sXgAZx0DA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zjDyKQSsNn0",
        "outputId": "4739553d-3ef0-4d3e-b4cc-9a0696b37c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ham       0.99      0.96      0.98       422\n",
            "        spam       0.97      0.99      0.98       475\n",
            "\n",
            "    accuracy                           0.98       897\n",
            "   macro avg       0.98      0.98      0.98       897\n",
            "weighted avg       0.98      0.98      0.98       897\n",
            "\n",
            "F1 score: 0.9802\n",
            "Rappel (recall): 0.9916\n"
          ]
        }
      ],
      "source": [
        "# Entraîner le modèle\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Prédictions\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "\n",
        "# Calcul de la précision, du rappel et du F1 score\n",
        "precision = precision_score(y_test, y_pred, pos_label=\"spam\")\n",
        "recall = recall_score(y_test, y_pred, pos_label=\"spam\")\n",
        "f1 = f1_score(y_test, y_pred, pos_label=\"spam\")  # Calcul du F1 score\n",
        "\n",
        "# Affichage des résultats\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"F1 score: {f1:.4f}\")  # Affichage du F1 score\n",
        "print(f\"Rappel (recall): {recall:.4f}\")  # Affichage du rappel\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Phrase à tester\n",
        "sentence = \"A major is sending you refunding of 30$, Please reply with your bank account information.\"\n",
        "\n",
        "# Prétraitement de la phrase\n",
        "processed_sentence = preprocess_text(sentence)\n",
        "\n",
        "\n",
        "\n",
        "# Utilisation du modèle pour obtenir les probabilités\n",
        "probabilities = pipeline.predict_proba([processed_sentence])[0]\n",
        "\n",
        "# Affichage des résultats\n",
        "spam_probability = probabilities[1]  # Probabilité de la classe \"spam\" (indice 1)\n",
        "non_spam_probability = probabilities[0]  # Probabilité de la classe \"non-spam\" (indice 0)\n",
        "\n",
        "print(f\"Probabilité de SPAM : {spam_probability * 100:.2f}%\")\n",
        "print(f\"Probabilité de NON SPAM : {non_spam_probability * 100:.2f}%\")\n",
        "\n",
        "# Utilisation du modèle pour prédire\n",
        "prediction = pipeline.predict([processed_sentence])[0]\n",
        "\n",
        "# Affichage du résultat\n",
        "if prediction == \"spam\":\n",
        "    print(\"La phrase est classée comme : SPAM\")\n",
        "else:\n",
        "    print(\"La phrase est classée comme : NON SPAM\")\n"
      ],
      "metadata": {
        "id": "fxJ_EMhVLkk4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19f7c427-9b5a-40c5-f693-99305197dfff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probabilité de SPAM : 89.00%\n",
            "Probabilité de NON SPAM : 11.00%\n",
            "La phrase est classée comme : SPAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8TaM1G0ga_7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}